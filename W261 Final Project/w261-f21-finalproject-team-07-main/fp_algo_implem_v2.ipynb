{"cells":[{"cell_type":"markdown","source":["# Algorithm Implementation"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"40f19638-9cfa-41aa-a0c2-747049227104"}}},{"cell_type":"markdown","source":["This notebook explains the math behind Regularized Logistic Regression and implements it from scratch in a distributed way using Spark RDD.\n\nWe apply the algorithm to a small sample of the data (3 months) and then compare the results with the Logistic Regression method available at the pyspark.ml library.\n\n<a href='$./fp_main_notebook_final'>To return to main notebook click here</a>."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1e3be872-132d-462d-9a19-abeee6cf435c"}}},{"cell_type":"markdown","source":["## The Logistic function"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b546c9fc-53df-48d6-a346-42ca44244083"}}},{"cell_type":"markdown","source":["In binary logistic regression, our response variable \\\\(Y\\\\) is categorical and can assume only two values: 1 or 0 (e.g. a flight delays more than 15 minutes or not).\n\nOur predictions \\\\(\\hat{y}\\\\) are a function of \\\\(p(\\mathbf{x})\\\\), the conditional probability that a flight delays given \\\\(X=\\mathbf{x}\\\\):\n\n$$p(\\mathbf{x})=Pr(Y=1|X=\\mathbf{x})$$\n\n\\\\(p(\\mathbf{x})\\\\) is a probability, and so can assume any real value between [0,1].\n\nTo complete the classification task, we need to assume a threshold (e.g. 0.5) to map the predicted probability \\\\(p(\\mathbf{x})\\\\) to one of the actual labels \\\\(\\hat{y}\\\\) can assume (1 or 0):\n\n$$\n\\begin{dcases} \n  \\hat{y}=1 &\\text{if } p(\\mathbf{x})\\ge0.5 \\\\\\\n  \\hat{y}=0 &\\text{if } p(\\mathbf{x})<0.5 \n\\end{dcases}\n$$\n\nTo ensure \\\\(p(\\mathbf{x})\\\\) is always bounded between [0,1], we model it using the sigmoid function:\n\n$$p(\\mathbf{x}) = \\frac{1}{1+e^{-(w_0x_0+w_1x_1+...+w_px_p)}}$$ \n\nwhere \\\\(\\mathbf{w} = w_0, w_1, ..., w_p\\\\) is the weight vector of our model, and \\\\(\\mathbf{x} = x_0, x_1, ..., x_p \\\\) is our predictor vector. Note that we are using an 'augmented' notation for our weight and predictor vectors, where \\\\(w_0\\\\) refers to the bias term and \\\\(x_0\\\\) is always set equal to 1.\n\nAfter a bit of algebraic manipulation and after taking the logarithm of both sides we arrive at:\n\n$$log\\biggl(\\frac{p(\\mathbf{x})}{1-p(\\mathbf{x})}\\biggl)=w_0x_0+w_1x_1+...+w_px_p$$\n\nThis equation shows that the log odds of \\\\(p(\\mathbf{x})\\\\) is linear in \\\\(\\mathbf{x}\\\\)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1c01e109-a169-482e-b59d-e068cb08549a"}}},{"cell_type":"markdown","source":["## The Log Loss function regularized using Elastic Net"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e1ae8196-243a-4dc7-ae4b-c8f48b921744"}}},{"cell_type":"markdown","source":["For every parametric machine learning algorithm, we need to define a loss function which we want to minimize in order to determine the optimal parameters \\\\(\\mathbf\\{w}\\\\) of our model.\n\nIn the case of logistic regression, we use the Log Loss function. We added to it a regularization term, having opted to use Elastic net, a hybrid of L1 and L2 regularizations, which is the default implementation in the pyparsk MLLib and gives us flexibility to experiment with different regularizations as a function of hyperparameter \\\\(\\alpha\\\\).\n\nMathematically, we have:\n\n$$l(\\mathbf{w})=-\\frac{1}{n}\\sum_{i=1}^n\\biggl[y^{(i)}\\log(p(\\mathbf{x})^{(i)})+(1-y^{(i)})\\log(1-p(\\mathbf{x})^{(i)})\\biggl]+\\alpha\\biggl(\\frac{\\lambda}{n}||\\mathbf{w}||_1\\biggl)+(1-\\alpha)\\biggl(\\frac{\\lambda}{2n}||\\mathbf{w}||_2^2\\biggl), \\alpha\\in[0,1], \\lambda\\ge0$$"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"41958c3f-e305-46c4-bdbd-7de9ab43ab20"}}},{"cell_type":"markdown","source":["## The gradients of regularized Log Loss function"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2e421805-a81c-4a89-8e01-2c8b52f6135d"}}},{"cell_type":"markdown","source":["To apply the Gradient Descent algorithm, we need to compute the gradient of our loss function.\n\nThe gradient for our weight vector in all positions except the bias term (\\\\(w_0\\\\)) is defined as:\n\n$$\\nabla_{\\mathbf{w_k}}=\\frac{1}{n}\\biggl(p(\\mathbf{x}^{(i)})-y^{(i)}\\biggl)*\\mathbf{x}+\\frac{\\alpha\\lambda}{n} sign(\\mathbf{w})+\\frac{(1-\\alpha)\\lambda}{n}\\mathbf{w}, k \\in [1,2,...p]$$\n\nFor the bias term we have:\n\n$$\\nabla_{w_0}=\\frac{1}{n}\\biggl(p(\\mathbf{x}^{(i)})-y^{(i)}\\biggl)$$"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9fd32ab8-7dc8-4f1a-963f-1fdf65ef7669"}}},{"cell_type":"markdown","source":["# Algorithm implementation"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"affc4cc2-29fd-4048-9a73-61436e451990"}}},{"cell_type":"markdown","source":["## Set-up blob storage"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ec7f780e-6d85-4129-b363-5b4dd134ce80"}}},{"cell_type":"code","source":["# init script to create the blob URL\nblob_container = 'team07'\nstorage_account = 'team07'\nsecret_scope = 'team07'\nsecret_key = 'team07'\nblob_url = f'wasbs://{blob_container}@{storage_account}.blob.core.windows.net'\n\n# generates the SAS token\nspark.conf.set(\n  f'fs.azure.sas.{blob_container}.{storage_account}.blob.core.windows.net',\n  dbutils.secrets.get(scope = secret_scope, key = secret_key)\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d2561fd2-85d0-447e-81c1-b057a73d7bb4"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Run imports"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"07eb8b48-64b1-44de-a8f1-247cb27ebc53"}}},{"cell_type":"code","source":["# imports\nimport numpy as np\nimport pandas as pd\nimport random\nimport time\nimport matplotlib.pyplot as plt\nfrom itertools import chain\nfrom pyspark.sql import Row, Column\nfrom pyspark.sql.types import BooleanType\nfrom pyspark.sql.functions import *\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\nfrom pyspark.ml.feature import StandardScaler, Imputer\nfrom pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier, LinearSVC\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1820f41f-9581-4751-9e38-122eb3b3a26a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Import the joined data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"16645f5c-d4ff-4b70-989c-bcba205a3887"}}},{"cell_type":"markdown","source":["We have a check-point at the blob storage with the data ready for modelling."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"77d8b252-292e-4fd3-aa1a-8daefc7ef9c7"}}},{"cell_type":"code","source":["# Read clean dataset\nrawDataDF = spark.read.parquet(f'{blob_url}/airlines_data_latest_weather_3m_trimmed_v4')\n\n# Print out number of rows\nprint(str(rawDataDF.count()) + ' rows in the data.')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b004eff-f492-4f02-ab5a-c75c237f93ab"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Filter columns"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2859ee06-8eb3-49cd-8cf7-42537395d157"}}},{"cell_type":"markdown","source":["Filter only columns of interest."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f1b55c26-cca3-4b49-9835-b01d12a55548"}}},{"cell_type":"code","source":["# label and features of interest\ncols = ['DEP_DEL15', 'YEAR', 'QUARTER', 'MONTH', 'DAY_OF_MONTH', 'DAY_OF_WEEK', 'OP_CARRIER', 'ORIGIN', 'DEST', 'rolling_average', 'delay_2hrs_originport', 'delay_4hrs_originport', 'delay_8hrs_originport', 'delay_12hrs_originport', 'delay_2hrs_destport', 'delay_4hrs_destport', 'delay_8hrs_destport', 'delay_12hrs_destport', 'delay_2hrs_orgairline', 'delay_4hrs_orgairline', 'delay_8hrs_orgairline', 'delay_12hrs_orgairline', 'arrdelay_2hrs_originport', 'arrdelay_4hrs_originport', 'arrdelay_8hrs_originport', 'arrdelay_12hrs_originport', 'DEP_HOUR', 'Part_of_Day', 'WND_direction_angle', 'WND_speed', 'CIG_ceiling_height_dimension', 'VIS_distance_dimension', 'TMP_air_temperature', 'DEW_dew_point_temperature', 'SLP_sea_level_pressure']\n\n# filter cols of interest\nfilteredDataDF = rawDataDF.select(cols)\n\n# print out number of features\n# minus two to account for the response and the year variables (which will only be used for splitting)\nprint(str(len(filteredDataDF.columns)-2) + ' features in the data.')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c80813b3-cd37-4511-a78d-18e52d22e5ca"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Split train and test data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ba4449b9-ef16-4177-aa3c-efae2c86bea6"}}},{"cell_type":"code","source":["# set 2019 as the hold-out test set\nhold_out_variable = 'MONTH'\nhold_out_threshold = '3'\n\n# split train and test sets\ntrainDF = filteredDataDF.filter(filteredDataDF[hold_out_variable]!=hold_out_threshold).cache()\ntestDF = filteredDataDF.filter(filteredDataDF[hold_out_variable]==hold_out_threshold).cache()\n\n# print count of rows\ntrain_years = sorted([x[0] for x in trainDF.select(hold_out_variable).distinct().collect()])\ntest_years = sorted([x[0] for x in testDF.select(hold_out_variable).distinct().collect()])\nprint(f'{trainDF.count()} rows in the train data, representing {hold_out_variable.lower()}s: {str(train_years)[1:-1]}.')\nprint(f'{testDF.count()} rows in the test data, representing {hold_out_variable.lower()}s: {str(test_years)[1:-1]}.')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ef8014b0-52b1-44f5-88a1-a9942b9e443f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Oversample minority class"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"89f8823b-e7d9-4d8f-ac1d-8d065a777c88"}}},{"cell_type":"markdown","source":["Given our data is unbalanced (most of flights do not delay) we will do random oversampling in the minority class aiming to get to a 50-50% class balance in the training dataset."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"98b6bc49-e45c-4458-841a-0fdfe80b724a"}}},{"cell_type":"code","source":["# split the data given labels\nminor_df = trainDF.filter(col('DEP_DEL15')==1).cache()\nmajor_df = trainDF.filter(col('DEP_DEL15')==0).cache()\n\n# compute the ratio between on-time and delayed flights\nn_ontime = major_df.count()\nn_delays = minor_df.count()\nratio = n_ontime/n_delays\nprint('The ratio of on-time to delayed flights is of {:0.2f}:1'.format(ratio))\n\n# oversample the delayed flights\noversample_df = minor_df.sample(withReplacement=True, fraction=ratio, seed=123)\naugmentedTrainDF = major_df.unionAll(oversample_df)\naugmentedTrainDF.groupBy('DEP_DEL15').count().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d261085c-95a1-4ec5-a726-0a02464e0248"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Set the pipeline"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3bcab518-668b-42ca-a576-5da433ab1942"}}},{"cell_type":"code","source":["# define categorical and continuous variables\ncategoricals = ['DAY_OF_MONTH', 'DAY_OF_WEEK', 'DEP_HOUR', 'OP_CARRIER', 'ORIGIN', 'DEST', 'TMP_air_temperature', 'DEW_dew_point_temperature']\nnumerics = ['delay_2hrs_originport', 'delay_4hrs_originport', 'delay_8hrs_originport', 'delay_12hrs_originport', 'delay_2hrs_destport', 'delay_4hrs_destport', 'delay_8hrs_destport', 'delay_12hrs_destport', 'delay_2hrs_orgairline', 'delay_4hrs_orgairline', 'delay_8hrs_orgairline', 'delay_12hrs_orgairline', 'arrdelay_2hrs_originport', 'arrdelay_4hrs_originport', 'arrdelay_8hrs_originport', 'arrdelay_12hrs_originport', 'WND_direction_angle', 'WND_speed', 'CIG_ceiling_height_dimension', 'VIS_distance_dimension', 'SLP_sea_level_pressure']\n\n# define feature transformations\nindexer = map(lambda c: StringIndexer(inputCol=c, outputCol=c+'_idx', handleInvalid = 'keep'), categoricals)\nohes = map(lambda c: OneHotEncoder(inputCol=c+'_idx', outputCol=c+'_class'), categoricals)\nimputer = Imputer(strategy='median', inputCols = numerics, outputCols = numerics)\nfeature_cols = list(map(lambda c: c+'_idx', categoricals)) + numerics\nvassembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\nscaler = StandardScaler(inputCol='features', outputCol='scaledFeatures',\n                        withStd=True, withMean=True)\n\n# assemble the pipeline\nlr_transf_stages = list(indexer) + list(ohes) + [imputer] + [vassembler] + [scaler]\nlr_pipeline = Pipeline(stages=lr_transf_stages)\n\n# transform the data\ntransfTrainDF = lr_pipeline.fit(augmentedTrainDF).transform(augmentedTrainDF).select(['scaledfeatures', 'DEP_DEL15'])\ntransfTestDF = lr_pipeline.fit(augmentedTrainDF).transform(testDF).select(['scaledfeatures', 'DEP_DEL15'])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"813538c1-8fd6-41a9-bdd4-0c29c7e86e33"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Cast the data as RDD"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"70ed2ec7-7af3-46aa-a2c8-bbcaa018fbc1"}}},{"cell_type":"code","source":["# Cast as RDD where records are tuples of (features_array, y)\ntrainRDD = transfTrainDF.rdd.map(lambda x: (np.array(x[:-1]), x[-1])).cache()\ntestRDD = transfTestDF.rdd.map(lambda x: (np.array(x[:-1]), x[-1])).cache()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5cf1a715-1284-4ba5-8694-e4cfdd960306"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Initialize baseline model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7f778244-f65b-4a29-ba50-94ca7214b417"}}},{"cell_type":"code","source":["num_features = trainRDD.map(lambda x: x[0].shape[1]).take(1)\nmean_prob_delay = trainRDD.map(lambda x: x[1]).mean()\nlog_odds = np.log(mean_prob_delay/(1-mean_prob_delay))\nwInit = np.concatenate((np.array([log_odds]), np.zeros(num_features)))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7de558e8-f6bd-4eef-a764-b00f7ef98c64"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Define Sigmoid function"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5e6c0a87-39bc-4697-9471-2031d79947eb"}}},{"cell_type":"code","source":["# helper function to compute the probability of positive class\ndef Sigmoid(dataRDD, W):\n    \"\"\"\n    Compute the conditional probability P(y = 1 | X; W).\n    Args:\n        dataRDD - each record is a tuple of (features_array, y)\n        W       - (array) model coefficients with bias at index 0\n    Returns:\n        predRDD - records are tuples of (features_array (true_y, prob_y))\n    \"\"\"\n    predRDD = dataRDD.map(lambda x: (np.append([1.0], x[0]), x[1])) \\\n                     .map(lambda x: (x[0], (x[1], np.power(1.0 + np.exp(-W.dot(x[0])), -1))))\n    return predRDD"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"88529f2d-fa27-4c00-9752-6322c97f5eb5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Define Predict function"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8c90c4d5-8b9a-445d-8c07-9c213ed6b92b"}}},{"cell_type":"code","source":["# helper function to predict class\ndef Predict(dataRDD, W, threshold=0.5):\n    \"\"\"\n    Helper function for predicting class.\n    Args:\n        dataRDD - each record is a tuple of (features_array, y)\n        W       - (array) model coefficients with bias at index 0\n        threshold - (float) minimum probability to assign class 1; defaults to 0.5\n    Returns:\n        predRDD - records are tuples of (features_array (true_y, pred_y))\n    \"\"\"\n    predRDD = Sigmoid(dataRDD, W).mapValues(lambda x: (x[0], 1 if x[1]>=threshold else 0))\n    return predRDD"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e4c97dec-9abc-4c3e-8fd9-bc33db2f2751"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Define Regularized Log Loss function"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"18d1ef54-555c-4861-94e3-4538a3cc4079"}}},{"cell_type":"code","source":["# helper function to compute regularized log loss\ndef LogLoss_wReg(dataRDD, W, elasticNetParam = 0.5, regParam = 0.1):\n    \"\"\"\n    Compute regularized log loss error.\n    Args:\n        dataRDD - each record is a tuple of (features_array, y)\n        W       - (array) model coefficients with bias at index 0\n    \"\"\"\n    logloss, n = Sigmoid(dataRDD, W).map(lambda x: (x[1][0]*np.log(x[1][1])+(1-x[1][0])*np.log(1-x[1][1]), 1)) \\\n                                    .reduce(lambda x,y: (x[0]+y[0], x[1]+y[1]))\n    lossterm = -logloss/n  \n    np.append([0.0], W[1:]) # the bias is not included in the regularization term\n    regterm = elasticNetParam*regParam/n*np.absolute(W).sum() + (1-elasticNetParam)*regParam/(2*n)*W.dot(W)\n    logloss_wReg = lossterm + regterm\n    return logloss_wReg"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0a8ac22a-8723-4315-b046-9c3e381b485c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Define Gradient Descent with Elastic Net Regularization"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c738b45d-053f-432d-9e2d-f3ebeda9f900"}}},{"cell_type":"code","source":["# helper function to perform one gradient descent step with regularization\ndef GDUpdate_wReg(dataRDD, W, learningRate = 0.1, elasticNetParam = 0.5, regParam = 0.1):\n    \"\"\"\n    Perform one gradient descent step/update with elastic net regularization.\n    Args:\n        dataRDD - tuple of (features_array, y)\n        W       - (array) model coefficients with bias at index 0\n        learningRate - (float) defaults to 0.1\n        elasticNetParam - (float) defaults to 0.5\n        regParam - (float) regularization term coefficient\n    Returns:\n        model   - (array) updated coefficients, bias still at index 0\n    \"\"\"\n    # unregularized gradient\n    unreg_grad, n = Sigmoid(dataRDD, W).map(lambda x: (x[0]*(x[1][1]-x[1][0]), 1)) \\\n                                       .reduce(lambda x,y: (x[0]+y[0], x[1]+y[1]))\n    # regularized gredient\n    W_ = np.append([0.0], W[1:]) # gradient of regularization term doesn't apply to bias\n    reg_grad = unreg_grad/n + elasticNetParam*regParam*np.sign(W_)/n + (1-elasticNetParam)*regParam*W_/n\n    \n    # update model\n    new_model = W - learningRate*reg_grad\n    \n    return new_model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"475899b2-c941-426e-82d5-85a8d76c3cf2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# helper function to compute gradient descent function\ndef GradientDescent_wReg(trainRDD, testRDD, wInit, nSteps = 30, learningRate = 0.1,\n                         elasticNetParam = 0.5, regParam = 0.1, verbose = False):\n    \"\"\"\n    Perform nSteps iterations of regularized gradient descent and \n    track loss on a test and train set. Return lists of\n    test/train loss and the models themselves.\n    \"\"\"\n    # initialize lists to track model performance\n    train_history, test_history, model_history = [], [], []\n    \n    # perform n updates & compute test and train loss after each epoch\n    model = wInit\n    for idx in range(nSteps):  \n        # update the model\n        model = GDUpdate_wReg(trainRDD, model, learningRate, elasticNetParam, regParam)\n        \n        # keep track of test/train loss for plotting\n        training_loss = LogLoss_wReg(trainRDD, model, elasticNetParam, regParam)\n        test_loss = LogLoss_wReg(testRDD, model, elasticNetParam, regParam)\n        train_history.append(training_loss)\n        test_history.append(test_loss)\n        model_history.append(model)\n        \n        # console output if desired\n        if verbose:\n            print('-'*50)\n            print(f'STEP: {idx+1}')\n            print(f'training loss: {training_loss}')\n            print(f'test loss: {test_loss}')\n            print(f'Model: {[w for w in model]}')\n            \n    return train_history, test_history, model_history"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"818587c6-8ce4-4e83-9802-ede9dbacb469"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# helper function to plot error curves\ndef plotErrorCurves(trainLoss, testLoss, title = None):\n    \"\"\"\n    Helper function for plotting.\n    Args: trainLoss (list of losses) , testLoss (list of losses)\n    \"\"\"\n    fig, ax = plt.subplots(1,1,figsize=(16,8))\n    x = list(range(len(trainLoss)+1))[1:]\n    ax.plot(x, trainLoss, 'k--', label='Training Loss')\n    ax.plot(x, testLoss, 'r--', label='Test Loss')\n    ax.legend(loc='upper right', fontsize='x-large')\n    plt.xlabel('Number of Iterations')\n    plt.ylabel('Log Loss')\n    if title:\n        plt.title(title)\n    plt.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"44b02087-1028-4703-b1b1-5d9a477d6a79"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Define Confusion Matrix"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"15bd939b-045d-4c03-a106-e8341ed6d2fb"}}},{"cell_type":"code","source":["# helper function to compute the confusion matrix\ndef ConfusionMatrix(dataRDD, W, threshold=0.5):\n  \"\"\"\n  Helper function to compute the confusion matrix.\n  Args:\n        dataRDD - each record is a tuple of (features_array, y)\n        W       - (array) model coefficients with bias at index 0\n        threshold - (float) minimum probability to assign class 1; defaults to 0.5\n    Returns:\n        ConfMatrix - (list) of [TP,FP,FN,TN]\n  \"\"\"\n  \n  def classify(row):\n    \"\"\"Helper function to perform classification row by row\"\"\"\n    if row[1]==1: #predicted class is 1\n      if row[0]==1: #actual class is 1\n        return 'TP'\n      if row[0]==0: #actual class is 0\n        return 'FP'\n    if row[1]==0: #predicted class is 0\n      if row[0]==1: #actual class is 1\n        return 'FN'\n      if row[0]==0: #actual class is 0\n        return 'TN'\n  \n  ConfMatrix = Predict(dataRDD, W, threshold).map(lambda x: (classify(x[1]),1)) \\\n                                             .reduceByKey(lambda x,y: x+y) \\\n                                             .collect()\n  return ConfMatrix"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9b4e1003-38c0-4afa-8fd1-0fdc7cb5a5a8"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# helper function to compute performance metrics\ndef metrics(dataRDD, W, threshold=0.5):\n  \"\"\"\n  Helper function to compute the confusion matrix.\n  Args:\n        dataRDD - each record is a tuple of (features_array, y)\n        W       - (array) model coefficients with bias at index 0\n        threshold - (float) minimum probability to assign class 1; defaults to 0.5\n    Returns:\n        perf_metrics - (list) of Accuracy, Weighted Precision, Weighted Recall,\n                       Weighted F-Score, Precision By Label, Recall By Label,\n                       F-Score By Label.\n  \"\"\" \n  cm = dict(ConfusionMatrix(dataRDD, W, threshold))\n  n = cm.get('TP',0)+cm.get('FP',0)+cm.get('FN',0)+cm.get('TN',0)\n  Accuracy = (cm.get('TP',0)+cm.get('TN',0))/n\n  PrecisionByLabel = [cm.get('TN',0)/(cm.get('FN',0)+cm.get('TN',0)), cm.get('TP',0)/(cm.get('TP',0)+cm.get('FP',0))]\n  RecallByLabel = [cm.get('TN',0)/(cm.get('FP',0)+cm.get('TN',0)), cm.get('TP',0)/(cm.get('TP',0)+cm.get('FN',0))]\n  WeightedPrecision = PrecisionByLabel[0]*(cm.get('FN',0)+cm.get('TN',0))/n + PrecisionByLabel[1]*(cm.get('TP',0)+cm.get('FP',0))/n\n  WeightedRecall = RecallByLabel[0]*(cm.get('FP',0)+cm.get('TN',0))/n + RecallByLabel[1]*(cm.get('TP',0)+cm.get('FN',0))/n\n  FScoreByLabel = list(2*np.array(PrecisionByLabel)*np.array(RecallByLabel)/(np.array(PrecisionByLabel)+np.array(RecallByLabel)))\n  WeightedFScore = 2*WeightedPrecision*WeightedRecall/(WeightedPrecision+WeightedRecall)\n  perf_metrics = [Accuracy, WeightedPrecision, WeightedRecall, WeightedFScore,\n                 PrecisionByLabel, RecallByLabel, FScoreByLabel]\n  \n  return perf_metrics"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a47b088a-23f0-48f1-940a-b17e454e5800"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Results (from scratch implementation)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c29423d3-f9b6-46bc-94c8-fcfe09abb723"}}},{"cell_type":"code","source":["start = time.time()\ntrain_history, test_history, model_history = GradientDescent_wReg(trainRDD, testRDD, wInit, nSteps = 50, learningRate = 0.1, \n                                                                  elasticNetParam = 0.5, regParam = 0.1, verbose = False)\nprint(f'Training time: {time.time() - start} seconds')\nresults = metrics(testRDD, W, threshold=0.5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a38b69be-755e-4e80-b21d-56508fb22ef2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["print('Performance metrics')\nprint('------------------------------------------------------------------------------------------------')\nprint(f'Accuracy: {results[0]}')\nprint(f'Weighted Precision: {results[1]}')\nprint(f'Weighted Recall: {results[2]}')\nprint(f'F-Score: {results[3]}')\nprint(f'Precision By Label: {results[4]}')\nprint(f'Recall By Label: {results[5]}')\nprint(f'F-Score by Label: {results[6]}')\nplotErrorCurves(train_history, test_history)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"725934f4-e260-4542-97ad-bad26d159b74"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Compare results with pyspark.ml.classification.LogisticRegression implementation"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cdfa23c3-ab48-4ec4-8f18-251e1718767b"}}},{"cell_type":"code","source":["# initialize Logistic Regression\nlr = LogisticRegression(featuresCol='scaledfeatures', labelCol='DEP_DEL15', maxIter=50, regParam=0.1, elasticNetParam=0.5, \n                        standardization=False, family='binomial')\n\n# fit the model\nlr_model = lr.fit(transfTrainDF)\n\n# assess performance metrics\nlr_summary = lr_model.evaluate(transfTestDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d84ce583-dd80-4898-be72-4ac84942d1de"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["print('Performance metrics')\nprint('------------------------------------------------------------------------------------------------')\nprint(f'Accuracy: {lr_summary.accuracy}')\nprint(f'Weighted Precision: {lr_summary.weightedPrecision}')\nprint(f'Weighted Recall: {lr_summary.weightedRecall}')\nprint(f'F-Score: {lr_summary.weightedFMeasure()}')\nprint(f'Precision By Label: {lr_summary.precisionByLabel}')\nprint(f'Recall By Label: {lr_summary.recallByLabel}')\nprint(f'F-Score by Label: {lr_summary.fMeasureByLabel()}')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eaf4c650-7425-4277-974e-9c73776ffdf3"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Conclusions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f104c1ab-9e2f-44d8-b58e-f5ae7f67e167"}}},{"cell_type":"markdown","source":["Our own implementation had a worst performance when compared to the pyspark.ml.classification.LogisticRegression implementation. We got a delay recall of 0.43 (vs 0.65 of pyspark implementation) and a delay precision of 0.29 (vs. 0.57 of pyspark implementation). We tried several adjustments to match the results but the differences continued to be large. We tried to keep the parameter sets the most comparable we could but there are many more parameters in the LogisticRegression class than in our own implementation. We are not sure as well that the pyspark implementation do use Batch Gradient Descent as the optimization algorithm under the hood. If it uses other algorithms (e.g. SGD or LBFGS) than it is understandable that results will be less comparable. The learning rate parameter is also a parameter that is not transparent in the pyspark implementation."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"892f1bd5-7cef-4a20-9a67-c7571090126a"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"fp_algo_implem_v2","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2164382306191133}},"nbformat":4,"nbformat_minor":0}
